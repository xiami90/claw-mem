#!/usr/bin/env python3
"""
OpenClaw è½»é‡åŒ–ä¸‰å±‚è®°å¿†æ¨¡å‹ - å‘é‡æœç´¢å¼•æ“
Vector Search Engine for OpenClaw Lite Memory System

@author: DataBot
@version: 1.0.0
@description: åŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½æ¨èç³»ç»Ÿ
"""

import numpy as np
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import pickle
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    content: str
    score: float
    category: str
    timestamp: datetime
    metadata: Dict[str, Any]
    id: str


@dataclass
class SearchSuggestion:
    """æœç´¢å»ºè®®æ•°æ®ç»“æ„"""
    suggestion: str
    relevance: float
    category: str
    reason: str


class VectorSearch:
    """
    å‘é‡æœç´¢å¼•æ“
    
    åŠŸèƒ½ç‰¹æ€§ï¼š
    - åŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢
    - æ™ºèƒ½ç›¸å…³æ€§æ’åº
    - ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨è
    - å®æ—¶ç´¢å¼•æ›´æ–°
    - å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—
    """
    
    def __init__(self, index_path: Optional[str] = None, dimension: int = 384):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            dimension: å‘é‡ç»´åº¦
        """
        self.index_path = Path(index_path) if index_path else Path(".memory/vectors.index")
        self.dimension = dimension
        self.index = {}
        self.metadata = {}
        self.word_vectors = {}
        
        # åˆ›å»ºç´¢å¼•ç›®å½•
        self.index_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æœç´¢å¼•æ“
        self._init_search_engine()
        
        logger.info(f"âœ… VectorSearch åˆå§‹åŒ–å®Œæˆ - ç»´åº¦: {dimension}, ç´¢å¼•è·¯å¾„: {self.index_path}")
    
    def _init_search_engine(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
        if self.index_path.exists():
            self._load_index()
        else:
            self._create_new_index()
        
        # åˆå§‹åŒ–è¯å‘é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self._init_word_vectors()
    
    def _create_new_index(self):
        """åˆ›å»ºæ–°ç´¢å¼•"""
        logger.info("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•")
        self.index = {
            "version": "1.0.0",
            "dimension": self.dimension,
            "created_at": datetime.now().isoformat(),
            "vectors": {},
            "metadata": {},
            "statistics": {
                "total_vectors": 0,
                "categories": {},
                "last_updated": datetime.now().isoformat()
            }
        }
    
    def _load_index(self):
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        try:
            with open(self.index_path, 'rb') as f:
                self.index = pickle.load(f)
            
            logger.info(f"ğŸ“‚ å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ - å‘é‡æ•°: {self.index['statistics']['total_vectors']}")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            self._create_new_index()
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        try:
            with open(self.index_path, 'wb') as f:
                pickle.dump(self.index, f)
            
            logger.info("ğŸ’¾ å‘é‡ç´¢å¼•ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•ä¿å­˜å¤±è´¥: {e}")
    
    def _init_word_vectors(self):
        """åˆå§‹åŒ–è¯å‘é‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è¯å‘é‡å®ç°
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡æ¨¡å‹
        
        # åŸºç¡€è¯å‘é‡è¯å…¸
        base_words = {
            # æŠ€æœ¯è¯æ±‡
            "æŠ€æœ¯": [0.8, 0.2, 0.1, 0.0] * 96,  # 384ç»´
            "ä»£ç ": [0.7, 0.3, 0.1, 0.0] * 96,
            "é¡¹ç›®": [0.6, 0.4, 0.2, 0.1] * 96,
            "å¼€å‘": [0.7, 0.2, 0.3, 0.0] * 96,
            "æ¶æ„": [0.8, 0.1, 0.4, 0.2] * 96,
            
            # å†³ç­–è¯æ±‡
            "å†³å®š": [0.9, 0.1, 0.8, 0.7] * 96,
            "é€‰æ‹©": [0.8, 0.2, 0.7, 0.6] * 96,
            "ç¡®å®š": [0.9, 0.1, 0.9, 0.8] * 96,
            "é‡‡ç”¨": [0.7, 0.3, 0.6, 0.5] * 96,
            
            # åå¥½è¯æ±‡
            "åå¥½": [0.8, 0.6, 0.7, 0.3] * 96,
            "å–œæ¬¢": [0.7, 0.7, 0.6, 0.2] * 96,
            "åˆé€‚": [0.6, 0.8, 0.5, 0.4] * 96,
            "æ›´å¥½": [0.7, 0.6, 0.8, 0.5] * 96,
            
            # é‡è¦æ€§è¯æ±‡
            "é‡è¦": [0.9, 0.2, 0.9, 0.9] * 96,
            "å…³é”®": [0.9, 0.1, 0.9, 0.8] * 96,
            "æ ¸å¿ƒ": [0.8, 0.2, 0.8, 0.7] * 96,
            "è®°ä½": [0.8, 0.3, 0.7, 0.8] * 96,
            
            # æ—¶é—´è¯æ±‡
            "è®¡åˆ’": [0.7, 0.5, 0.8, 0.6] * 96,
            "ç›®æ ‡": [0.8, 0.4, 0.9, 0.7] * 96,
            "ä¸‹ä¸€æ­¥": [0.6, 0.6, 0.7, 0.5] * 96,
            "å‡†å¤‡": [0.6, 0.5, 0.6, 0.4] * 96,
        }
        
        self.word_vectors = base_words
        logger.info(f"âœ… è¯å‘é‡åˆå§‹åŒ–å®Œæˆ - è¯æ±‡æ•°: {len(base_words)}")
    
    def text_to_vector(self, text: str) -> np.ndarray:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å‘é‡
        """
        if not text or not text.strip():
            return np.zeros(self.dimension)
        
        # ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–å®ç°
        words = self._tokenize(text)
        
        if not words:
            return np.zeros(self.dimension)
        
        # åŸºäºè¯å‘é‡çš„å¹³å‡æ± åŒ–
        vectors = []
        for word in words:
            if word in self.word_vectors:
                vectors.append(np.array(self.word_vectors[word]))
            else:
                # ä¸ºæœªçŸ¥è¯ç”Ÿæˆéšæœºå‘é‡
                np.random.seed(hash(word) % 2**32)
                random_vector = np.random.randn(self.dimension) * 0.1
                vectors.append(random_vector)
        
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(self.dimension)
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•çš„ä¸­æ–‡åˆ†è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        import re
        text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', '', text)
        
        # ç®€å•çš„åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œå¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
        words = re.split(r'[\s\,\;\:]+', text)
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’ŒçŸ­è¯
        words = [word.strip() for word in words if word.strip() and len(word.strip()) > 1]
        
        return words
    
    def search(self, query: str, limit: int = 5, min_score: float = 0.1) -> List[Dict[str, Any]]:
        """
        å‘é‡æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” å‘é‡æœç´¢ - æŸ¥è¯¢: '{query}', é™åˆ¶: {limit}")
            
            if not self.index or not self.index['vectors']:
                logger.warning("å‘é‡ç´¢å¼•ä¸ºç©º")
                return []
            
            # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
            query_vector = self.text_to_vector(query)
            
            if np.all(query_vector == 0):
                logger.warning("æŸ¥è¯¢å‘é‡ä¸ºé›¶ï¼Œæ— æ³•æœç´¢")
                return []
            
            results = []
            
            # è®¡ç®—ä¸æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
            for vector_id, vector_data in self.index['vectors'].items():
                stored_vector = np.array(vector_data['vector'])
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(query_vector, stored_vector)
                
                if similarity >= min_score:
                    results.append({
                        "id": vector_id,
                        "content": vector_data['content'],
                        "score": float(similarity),
                        "metadata": vector_data.get('metadata', {}),
                        "timestamp": vector_data.get('timestamp', '')
                    })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x["score"], reverse=True)
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = results[:limit]
            
            logger.info(f"âœ… å‘é‡æœç´¢å®Œæˆ - æ‰¾åˆ° {len(final_results)} æ¡ç›¸å…³è®°å¿†")
            return final_results
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def add_vector(self, vector_id: str, content: str, metadata: Optional[Dict] = None) -> bool:
        """
        æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        
        Args:
            vector_id: å‘é‡ID
            content: å†…å®¹æ–‡æœ¬
            metadata: å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        try:
            # å°†å†…å®¹è½¬æ¢ä¸ºå‘é‡
            vector = self.text_to_vector(content)
            
            if np.all(vector == 0):
                logger.warning(f"å†…å®¹ '{content}' è½¬æ¢ä¸ºé›¶å‘é‡ï¼Œè·³è¿‡")
                return False
            
            # æ·»åŠ åˆ°ç´¢å¼•
            self.index['vectors'][vector_id] = {
                'vector': vector.tolist(),
                'content': content,
                'metadata': metadata or {},
                'timestamp': datetime.now().isoformat()
            }
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.index['statistics']['total_vectors'] += 1
            
            logger.info(f"âœ… å‘é‡æ·»åŠ æˆåŠŸ: {vector_id}")
            return True
            
        except Exception as e:
            logger.error(f"å‘é‡æ·»åŠ å¤±è´¥: {e}")
            return False
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            # è®¡ç®—ç‚¹ç§¯
            dot_product = np.dot(vec1, vec2)
            
            # è®¡ç®—èŒƒæ•°
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = dot_product / (norm1 * norm2)
            
            # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0