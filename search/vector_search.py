#!/usr/bin/env python3
"""
OpenClaw è½»é‡åŒ–ä¸‰å±‚è®°å¿†æ¨¡å‹ - å‘é‡æœç´¢å¼•æ“
Vector Search Engine for OpenClaw Lite Memory System

@author: DataBot
@version: 1.0.0
@description: åŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½æ¨èç³»ç»Ÿ
"""

import numpy as np
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import pickle
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    content: str
    score: float
    category: str
    timestamp: datetime
    metadata: Dict[str, Any]
    id: str


@dataclass
class SearchSuggestion:
    """æœç´¢å»ºè®®æ•°æ®ç»“æ„"""
    suggestion: str
    relevance: float
    category: str
    reason: str


class VectorSearch:
    """
    å‘é‡æœç´¢å¼•æ“
    
    åŠŸèƒ½ç‰¹æ€§ï¼š
    - åŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢
    - æ™ºèƒ½ç›¸å…³æ€§æ’åº
    - ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨è
    - å®æ—¶ç´¢å¼•æ›´æ–°
    - å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—
    """
    
    def __init__(self, index_path: Optional[str] = None, dimension: int = 384):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            dimension: å‘é‡ç»´åº¦
        """
        self.index_path = Path(index_path) if index_path else Path(".memory/vectors.index")
        self.dimension = dimension
        self.index = {}
        self.metadata = {}
        self.word_vectors = {}
        
        # åˆ›å»ºç´¢å¼•ç›®å½•
        self.index_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æœç´¢å¼•æ“
        self._init_search_engine()
        
        logger.info(f"âœ… VectorSearch åˆå§‹åŒ–å®Œæˆ - ç»´åº¦: {dimension}, ç´¢å¼•è·¯å¾„: {self.index_path}")
    
    def _init_search_engine(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
        if self.index_path.exists():
            self._load_index()
        else:
            self._create_new_index()
        
        # åˆå§‹åŒ–è¯å‘é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self._init_word_vectors()
    
    def _create_new_index(self):
        """åˆ›å»ºæ–°ç´¢å¼•"""
        logger.info("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•")
        self.index = {
            "version": "1.0.0",
            "dimension": self.dimension,
            "created_at": datetime.now().isoformat(),
            "vectors": {},
            "metadata": {},
            "statistics": {
                "total_vectors": 0,
                "categories": {},
                "last_updated": datetime.now().isoformat()
            }
        }
    
    def _load_index(self):
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        try:
            with open(self.index_path, 'rb') as f:
                self.index = pickle.load(f)
            
            logger.info(f"ğŸ“‚ å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ - å‘é‡æ•°: {self.index['statistics']['total_vectors']}")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            self._create_new_index()
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        try:
            with open(self.index_path, 'wb') as f:
                pickle.dump(self.index, f)
            
            logger.info("ğŸ’¾ å‘é‡ç´¢å¼•ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•ä¿å­˜å¤±è´¥: {e}")
    
    def _init_word_vectors(self):
        """åˆå§‹åŒ–è¯å‘é‡ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è¯å‘é‡å®ç°
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡æ¨¡å‹
        
        # åŸºç¡€è¯å‘é‡è¯å…¸
        base_words = {
            # æŠ€æœ¯è¯æ±‡
            "æŠ€æœ¯": [0.8, 0.2, 0.1, 0.0] * 96,  # 384ç»´
            "ä»£ç ": [0.7, 0.3, 0.1, 0.0] * 96,
            "é¡¹ç›®": [0.6, 0.4, 0.2, 0.1] * 96,
            "å¼€å‘": [0.7, 0.2, 0.3, 0.0] * 96,
            "æ¶æ„": [0.8, 0.1, 0.4, 0.2] * 96,
            
            # å†³ç­–è¯æ±‡
            "å†³å®š": [0.9, 0.1, 0.8, 0.7] * 96,
            "é€‰æ‹©": [0.8, 0.2, 0.7, 0.6] * 96,
            "ç¡®å®š": [0.9, 0.1, 0.9, 0.8] * 96,
            "é‡‡ç”¨": [0.7, 0.3, 0.6, 0.5] * 96,
            
            # åå¥½è¯æ±‡
            "åå¥½": [0.8, 0.6, 0.7, 0.3] * 96,
            "å–œæ¬¢": [0.7, 0.7, 0.6, 0.2] * 96,
            "åˆé€‚": [0.6, 0.8, 0.5, 0.4] * 96,
            "æ›´å¥½": [0.7, 0.6, 0.8, 0.5] * 96,
            
            # é‡è¦æ€§è¯æ±‡
            "é‡è¦": [0.9, 0.2, 0.9, 0.9] * 96,
            "å…³é”®": [0.9, 0.1, 0.9, 0.8] * 96,
            "æ ¸å¿ƒ": [0.8, 0.2, 0.8, 0.7] * 96,
            "è®°ä½": [0.8, 0.3, 0.7, 0.8] * 96,
            
            # æ—¶é—´è¯æ±‡
            "è®¡åˆ’": [0.7, 0.5, 0.8, 0.6] * 96,
            "ç›®æ ‡": [0.8, 0.4, 0.9, 0.7] * 96,
            "ä¸‹ä¸€æ­¥": [0.6, 0.6, 0.7, 0.5] * 96,
            "å‡†å¤‡": [0.6, 0.5, 0.6, 0.4] * 96,
        }
        
        self.word_vectors = base_words
        logger.info(f"âœ… è¯å‘é‡åˆå§‹åŒ–å®Œæˆ - è¯æ±‡æ•°: {len(base_words)}")
    
    def text_to_vector(self, text: str) -> np.ndarray:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å‘é‡
        """
        if not text or not text.strip():
            return np.zeros(self.dimension)
        
        # ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–å®ç°
        words = self._tokenize(text)
        
        if not words:
            return np.zeros(self.dimension)
        
        # åŸºäºè¯å‘é‡çš„å¹³å‡æ± åŒ–
        vectors = []
        for word in words:
            if word in self.word_vectors:
                vectors.append(np.array(self.word_vectors[word]))
            else:
                # ä¸ºæœªçŸ¥è¯ç”Ÿæˆéšæœºå‘é‡
                np.random.seed(hash(word) % 2**32)
                random_vector = np.random.randn(self.dimension) * 0.1
                vectors.append(random_vector)
        
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(self.dimension)
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•çš„ä¸­æ–‡åˆ†è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        import re
        text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', '', text)