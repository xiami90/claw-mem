#!/usr/bin/env python3
"""
OpenClaw è½»é‡åŒ–ä¸‰å±‚è®°å¿†æ¨¡å‹ - æ™ºèƒ½ä¼šè¯æ•è·å™¨
Smart Session Capture for OpenClaw Lite Memory System

@author: DataBot
@version: 1.0.0
@description: æ™ºèƒ½è¯†åˆ«å’Œæ•è·ä¼šè¯ä¸­çš„é‡è¦ä¿¡æ¯
"""

import re
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CaptureType(Enum):
    """æ•è·ç±»å‹æšä¸¾"""
    DECISION = "decision"
    PREFERENCE = "preference" 
    FACT = "fact"
    PLAN = "plan"
    LESSON = "lesson"
    WARNING = "warning"
    CONTACT = "contact"
    GENERAL = "general"


@dataclass
class CapturedItem:
    """æ•è·é¡¹æ•°æ®ç»“æ„"""
    type: CaptureType
    content: str
    confidence: float
    timestamp: datetime
    context: str
    metadata: Dict[str, Any]


class SmartSessionCapture:
    """
    æ™ºèƒ½ä¼šè¯æ•è·å™¨
    
    åŠŸèƒ½ç‰¹æ€§ï¼š
    - å¤šæ¨¡å¼é‡è¦ä¿¡æ¯è¯†åˆ«
    - ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ™ºèƒ½æå–
    - ç½®ä¿¡åº¦è¯„ä¼°å’Œè¿‡æ»¤
    - å®æ—¶æ•è·å’Œæ‰¹å¤„ç†
    - å¯æ‰©å±•çš„æ¨¡å¼åŒ¹é…
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æ•è·å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or self._get_default_config()
        self.capture_patterns = self._load_capture_patterns()
        self.context_window = []
        self.max_context_size = self.config.get("max_context_size", 10)
        self.min_confidence = self.config.get("min_confidence", 0.6)
        
        logger.info("âœ… SmartSessionCapture åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "max_context_size": 10,
            "min_confidence": 0.6,
            "enable_realtime_capture": True,
            "enable_batch_processing": True,
            "patterns": {
                "decision": {"weight": 0.9, "min_confidence": 0.7},
                "preference": {"weight": 0.8, "min_confidence": 0.6},
                "fact": {"weight": 0.7, "min_confidence": 0.5},
                "plan": {"weight": 0.8, "min_confidence": 0.6},
                "lesson": {"weight": 0.85, "min_confidence": 0.65}
            }
        }
    
    def _load_capture_patterns(self) -> Dict[CaptureType, List[Tuple[str, float, Dict[str, Any]]]]:
        """åŠ è½½æ•è·æ¨¡å¼"""
        patterns = {
            CaptureType.DECISION: [
                # ä¸­æ–‡å†³ç­–æ¨¡å¼
                (r"(?:å†³å®š|å†³ç­–|é€‰æ‹©|ç¡®å®š|é‡‡ç”¨|ä½¿ç”¨|åº”ç”¨)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.9, {"language": "zh", "formality": "high"}),
                (r"(?:æˆ‘ä»¬|æˆ‘)\s*(?:å†³å®š|é€‰æ‹©)\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"language": "zh", "speaker": "inclusive"}),
                (r"(?:ç¡®å®š|æ•²å®š)\s*(?:äº†|è¦)\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"language": "zh", "certainty": "high"}),
                
                # æŠ€æœ¯å†³ç­–
                (r"(?:æŠ€æœ¯æ ˆ|æ¡†æ¶|å·¥å…·|æ–¹æ¡ˆ)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"domain": "tech", "type": "stack"}),
                (r"(?:ç”¨|ä½¿ç”¨)\s*(.+?)\s*(?:åš|å¼€å‘|å®ç°)", 0.8, {"domain": "tech", "type": "implementation"}),
                (r"(?:æ¶æ„|è®¾è®¡)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"domain": "tech", "type": "architecture"}),
            ],
            
            CaptureType.PREFERENCE: [
                # åå¥½è¡¨è¾¾
                (r"(?:åå¥½|å–œæ¬¢|åçˆ±|å€¾å‘äº|æ›´å–œæ¬¢)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.9, {"type": "positive_preference"}),
                (r"(?:ä¸å–œæ¬¢|è®¨åŒ|åæ„Ÿ|ä¸æƒ³|ä¸è¦)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "negative_preference"}),
                (r"(?:æˆ‘è§‰å¾—|æˆ‘è®¤ä¸º|å¯¹æˆ‘è€Œè¨€)\s*(.+?)\s*(?:æ›´å¥½|æ›´åˆé€‚|æ›´å–œæ¬¢)", 0.8, {"speaker": "personal", "type": "opinion"}),
                (r"(?:ç”¨æˆ·|æˆ‘ä»¬)\s*(?:åå¥½|å–œæ¬¢)\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"speaker": "collective", "type": "preference"}),
            ],
            
            CaptureType.FACT: [
                # é‡è¦äº‹å®
                (r"(?:é‡è¦|å…³é”®|æ ¸å¿ƒ|ä¸»è¦)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"importance": "high"}),
                (r"(?:è®°ä½|ç‰¢è®°|å¤‡å¿˜|è®°å½•)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.9, {"action": "remember", "importance": "critical"}),
                (r"(?:äº‹å®æ˜¯|å®é™…ä¸Šæ˜¯|å…¶å®æ˜¯)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"type": "fact_statement", "certainty": "high"}),
                (r"(?:ç¡®è®¤|è¯å®|éªŒè¯)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "confirmation", "reliability": "high"}),
                
                # æŠ€æœ¯äº‹å®
                (r"(?:ç‰ˆæœ¬|å‹å·|è§„æ ¼|é…ç½®)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"domain": "tech", "type": "specification"}),
                (r"(?:API|æ¥å£|ç«¯ç‚¹)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"domain": "tech", "type": "api_info"}),
            ],
            
            CaptureType.PLAN: [
                # è®¡åˆ’å’Œç›®æ ‡
                (r"(?:è®¡åˆ’|æ‰“ç®—|å‡†å¤‡|å°†è¦|ä¸‹ä¸€æ­¥)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "plan", "timeframe": "future"}),
                (r"(?:ç›®æ ‡|ç›®çš„|è¦è¾¾æˆ|è¦å®Œæˆ)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "goal", "timeframe": "future"}),
                (r"(?:æ˜å¤©|åå¤©|ä¸‹å‘¨|ä¸‹ä¸ªæœˆ)\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"type": "scheduled", "timeframe": "specific"}),
                (r"(?:ç¬¬ä¸€æ­¥|ç¬¬äºŒæ­¥|ç¬¬ä¸‰é˜¶æ®µ)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "step", "structure": "sequential"}),
            ],
            
            CaptureType.LESSON: [
                # ç»éªŒæ•™è®­
                (r"(?:ç»éªŒ|æ•™è®­|å­¦åˆ°|æ€»ç»“|åæ€)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "lesson_learned"}),
                (r"(?:åº”è¯¥|ä¸åº”è¯¥|è¦|ä¸è¦)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"type": "advice", "recommendation": "prescriptive"}),
                (r"(?:æ³¨æ„|å°å¿ƒ|è­¦æƒ•)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "warning", "urgency": "high"}),
                (r"(?:é”™è¯¯|å¤±è¯¯|é—®é¢˜)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"type": "mistake", "sentiment": "negative"}),
            ],
            
            CaptureType.WARNING: [
                # è­¦å‘Šå’Œé£é™©
                (r"(?:è­¦å‘Š|é£é™©|å±é™©|æ³¨æ„)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.9, {"type": "warning", "severity": "high"}),
                (r"(?:å¯èƒ½|ä¹Ÿè®¸|å¤§æ¦‚)\s*(?:ä¼š|è¦)\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.7, {"type": "possibility", "certainty": "medium"}),
                (r"(?:å¦‚æœ|å‡å¦‚|å€˜è‹¥)\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.75, {"type": "condition", "structure": "hypothetical"}),
            ],
            
            CaptureType.CONTACT: [
                # è”ç³»ä¿¡æ¯
                (r"(?:è”ç³»äºº|è”ç³»æ–¹å¼|é‚®ç®±|ç”µè¯)[ï¼š:]\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.85, {"type": "contact_info"}),
                (r"(?:@|è”ç³»)\s*(\w+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)", 0.8, {"type": "mention", "platform": "generic"}),
            ]
        }
        
        return patterns
    
    def capture_from_text(self, text: str, context: Optional[str] = None) -> List[CapturedItem]:
        """
        ä»æ–‡æœ¬ä¸­æ•è·é‡è¦ä¿¡æ¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            æ•è·é¡¹åˆ—è¡¨
        """
        if not text or not text.strip():
            return []
        
        logger.info(f"ğŸ¯ å¼€å§‹æ™ºèƒ½æ•è· - æ–‡æœ¬é•¿åº¦: {len(text)}, ä¸Šä¸‹æ–‡: {context}")
        
        # æ›´æ–°ä¸Šä¸‹æ–‡çª—å£
        self._update_context(text, context)
        
        captured_items = []
        
        # ä½¿ç”¨æ¨¡å¼åŒ¹é…æ•è·
        for capture_type, patterns in self.capture_patterns.items():
            for pattern, base_confidence, metadata in patterns:
                matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
                
                for match in matches:
                    if isinstance(match, tuple):
                        content = match[0] if match[0] else match[1] if len(match) > 1 else str(match)
                    else:
                        content = str(match)
                    
                    content = content.strip()
                    
                    # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªé•¿çš„å†…å®¹
                    if len(content) < 10 or len(content) > 500:
                        continue
                    
                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence = self._calculate_confidence(content, capture_type, base_confidence, text)
                    
                    if confidence >= self.min_confidence:
                        captured_item = CapturedItem(
                            type=capture_type,
                            content=content,
                            confidence=confidence,
                            timestamp=datetime.now(),
                            context=self._get_current_context(),
                            metadata={**metadata, "capture_method": "pattern_matching", "pattern": pattern}
                        )
                        
                        captured_items.append(captured_item)
                        logger.info(f"âœ… æ•è·æˆåŠŸ: [{capture_type.value}] {content[:50]}... (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # å¦‚æœæ²¡æœ‰æ•è·åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œä½¿ç”¨æ™ºèƒ½æå–
        if len(captured_items) < 2:
            smart_items = self._smart_extract(text, context)
            captured_items.extend(smart_items)
        
        # å»é‡å’Œåˆå¹¶
        captured_items = self._deduplicate_items(captured_items)
        
        logger.info(f"ğŸ¯ æ•è·å®Œæˆ - å…±æ•è· {len(captured_items)} æ¡é‡è¦ä¿¡æ¯")
        return captured_items
    
    def _update_context(self, text: str, context: Optional[str] = None):
        """æ›´æ–°ä¸Šä¸‹æ–‡çª—å£"""
        context_entry = {
            "text": text[:200],  # åªä¿ç•™å‰200å­—ç¬¦
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "length": len(text)
        }
        
        self.context_window.append(context_entry)
        
        # ä¿æŒä¸Šä¸‹æ–‡çª—å£å¤§å°
        if len(self.context_window) > self.max_context_size:
            self.context_window.pop(0)
    
    def _get_current_context(self) -> str:
        """è·å–å½“å‰ä¸Šä¸‹æ–‡"""
        if not self.context_window:
            return ""
        
        # è¿”å›æœ€è¿‘å‡ æ¡ä¸Šä¸‹æ–‡çš„æ‘˜è¦
        recent_contexts = self.context_window[-3:]  # æœ€è¿‘3æ¡
        context_summary = []
        
        for i, ctx in enumerate(recent_contexts):
            text_preview = ctx["text"][:50] + "..." if len(ctx["text"]) > 50 else ctx["text"]
            context_summary.append(f"{i+1}. {text_preview}")
        
        return "\n".join(context_summary)
    
    def _calculate_confidence(self, content: str, capture_type: CaptureType, base_confidence: float, full_text: str) -> float:
        """è®¡ç®—æ•è·ç½®ä¿¡åº¦"""
        confidence = base_confidence
        
        # é•¿åº¦å› å­
        content_length = len(content)
        if 20 <= content_length <= 100:
            confidence += 0.05
        elif content_length > 200:
            confidence -= 0.1
        elif content_length < 15:
            confidence -= 0.15
        
        # å…³é”®è¯å¯†åº¦å› å­
        important_words = {
            CaptureType.DECISION: ["å†³å®š", "é€‰æ‹©", "ç¡®å®š", "é‡‡ç”¨", "ä½¿ç”¨"],
            CaptureType.PREFERENCE: ["åå¥½", "å–œæ¬¢", "æ›´åˆé€‚", "æ›´å¥½"],
            CaptureType.FACT: ["é‡è¦", "å…³é”®", "è®°ä½", "äº‹å®", "ç¡®è®¤"],
            CaptureType.PLAN: ["è®¡åˆ’", "ç›®æ ‡", "ä¸‹ä¸€æ­¥", "å‡†å¤‡"],
            CaptureType.LESSON: ["ç»éªŒ", "æ•™è®­", "å­¦åˆ°", "æ€»ç»“", "åº”è¯¥"]
        }
        
        if capture_type in important_words:
            keyword_count = sum(1 for word in important_words[capture_type] if word in content)
            confidence += min(keyword_count * 0.03, 0.15)
        
        # ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å› å­
        if self._check_context_consistency(content, capture_type):
            confidence += 0.1
        
        # ä½ç½®å› å­ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å†…å®¹é€šå¸¸æ›´é‡è¦ï¼‰
        position_ratio = full_text.find(content) / len(full_text) if full_text else 0.5
        if position_ratio < 0.2 or position_ratio > 0.8:  # å¼€å¤´æˆ–ç»“å°¾
            confidence += 0.05
        
        # ç¡®ä¿ç½®ä¿¡åº¦åœ¨åˆç†èŒƒå›´å†…
        return max(0.1, min(1.0, confidence))
    
    def _check_context_consistency(self, content: str, capture_type: CaptureType) -> bool:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸€è‡´æ€§"""
        if not self.context_window:
            return False
        
        # æ£€æŸ¥æœ€è¿‘å‡ æ¡ä¸Šä¸‹æ–‡æ˜¯å¦ä¸å½“å‰æ•è·ç±»å‹ä¸€è‡´
        recent_types = []
        for ctx in self.context_window[-3:]:
            # ç®€å•çš„ç±»å‹æ¨æ–­
            if "å†³å®š" in ctx["text"] or "é€‰æ‹©" in ctx["text"]:
                recent_types.append(CaptureType.DECISION)
            elif "åå¥½" in ctx["text"] or "å–œæ¬¢" in ctx["text"]:
                recent_types.append(CaptureType.PREFERENCE)
            elif "è®¡åˆ’" in ctx["text"] or "ç›®æ ‡" in ctx["text"]:
                recent_types.append(CaptureType.PLAN)
        
        # å¦‚æœæœ€è¿‘ä¸Šä¸‹æ–‡ä¸å½“å‰æ•è·ç±»å‹ä¸€è‡´ï¼Œè¿”å›True
        return capture_type in recent_types[-2:] if len(recent_types) >= 2 else False
    
    def _smart_extract(self, text: str, context: Optional[str] = None) -> List[CapturedItem]:
        """æ™ºèƒ½æå–é‡è¦ä¿¡æ¯"""
        # å½“æ¨¡å¼åŒ¹é…æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿå†…å®¹æ—¶çš„å¤‡ç”¨æå–æ–¹æ³•
        
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        extracted_items = []
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            important_keywords = {
                "é‡è¦": CaptureType.FACT,
                "å†³å®š": CaptureType.DECISION,
                "è®¡åˆ’": CaptureType.PLAN,
                "åå¥½": CaptureType.PREFERENCE,
                "ç»éªŒ": CaptureType.LESSON,
                "åº”è¯¥": CaptureType.LESSON
            }
            
            for keyword, capture_type in important_keywords.items():
                if keyword in sentence:
                    confidence = 0.6  # åŸºç¡€ç½®ä¿¡åº¦
                    
                    # ä½ç½®æƒé‡
                    if i == 0 or i == len(sentences) - 1:  # å¼€å¤´æˆ–ç»“å°¾
                        confidence += 0.1
                    
                    captured_item = CapturedItem(
                        type=capture_type,
                        content=sentence,
                        confidence=confidence,
                        timestamp=datetime.now(),
                        context=self._get_current_context(),
                        metadata={"capture_method": "smart_extraction", "sentence_index": i}
                    )
                    
                    extracted_items.append(captured_item)
                    logger.info(f"âœ… æ™ºèƒ½æå–: [{capture_type.value}] {sentence[:50]}... (ç½®ä¿¡åº¦: {confidence:.2f})")
                    break
        
        return extracted_items
    
    def _deduplicate_items(self, items: List[CapturedItem]) -> List[CapturedItem]:
        """å»é‡æ•è·é¡¹"""
        if not items:
            return []
        
        unique_items = []
        seen_content = set()
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œä¿ç•™é«˜ç½®ä¿¡åº¦çš„
        items.sort(key=lambda x: x.confidence, reverse=True)
        
        for item in items:
            # åˆ›å»ºå†…å®¹æŒ‡çº¹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            content_fingerprint = self._create_content_fingerprint(item.content)
            
            if content_fingerprint not in seen_content:
                seen_content.add(content_fingerprint)
                unique_items.append(item)
            else:
                logger.info(f"ğŸ”„ å»é‡: è·³è¿‡é‡å¤å†…å®¹ - {item.content[:50]}...")
        
        return unique_items
    
    def _create_content_fingerprint(self, content: str) -> str:
        """åˆ›å»ºå†…å®¹æŒ‡çº¹ç”¨äºå»é‡"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œè½¬æ¢ä¸ºå°å†™
        import re
        clean_content = re.sub(r'[^\w]', '', content.lower())
        
        # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œè¿”å›åŸæ–‡
        if len(clean_content) < 10:
            return content
        
        # è¿”å›å‰20ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        return clean_content[:20]
